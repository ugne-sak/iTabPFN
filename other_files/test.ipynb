{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests & trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([128, 20])\n",
      "output shape: torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30) # torch.nn.Linear(in_features, out_features)\n",
    "input = torch.randn(128, 20) # 128 vecs, each of length 20\n",
    "output = m(input)\n",
    "print(\"input shape:\", input.shape)\n",
    "print(\"output shape:\", output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding()\n",
    "\n",
    "* num_embeddings (int) – size of the dictionary of embeddings\n",
    "* embedding_dim (int) – the size of each embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 6])\n",
      "output shape: torch.Size([2, 6, 3])\n",
      "input: tensor([[1, 2, 4, 5, 7, 7],\n",
      "        [4, 3, 2, 9, 2, 2]])\n",
      "output: tensor([[[ 0.2720, -0.6043,  0.1841],\n",
      "         [-0.9251,  1.0612, -0.0828],\n",
      "         [-0.3585,  0.8105, -0.3548],\n",
      "         [ 0.4088, -0.8186,  1.6529],\n",
      "         [ 0.4062,  1.9184,  1.1697],\n",
      "         [ 0.4062,  1.9184,  1.1697]],\n",
      "\n",
      "        [[-0.3585,  0.8105, -0.3548],\n",
      "         [-0.6742, -0.0118,  0.5788],\n",
      "         [-0.9251,  1.0612, -0.0828],\n",
      "         [-1.4209,  0.0125, -0.7004],\n",
      "         [-0.9251,  1.0612, -0.0828],\n",
      "         [-0.9251,  1.0612, -0.0828]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = nn.Embedding(10, 3) # torch.nn.Embedding(num_embeddings, embedding_dim, ...)\n",
    "# a batch of 2 samples of 6 indices each\n",
    "input = torch.LongTensor([[1,2,4,5,7,7],[4,3,2,9,2,2]])\n",
    "output = embedding(input)\n",
    "print(\"input shape:\", input.shape)\n",
    "print(\"output shape:\", output.size())\n",
    "print(\"input:\", input)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[0, 2, 0, 5]])\n",
      "output: tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.4685, -1.4625,  1.0954],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6073, -0.6267,  0.5618]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# example with padding_idx\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "print(\"input:\", input)\n",
    "print(\"output:\", embedding(input))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "x = lambda a, b : a * b\n",
    "print(x(5, 6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_first = False\n",
    "src_key_padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not batch_first # AssertionError when batch_first=True: not True = False \n",
    "assert src_key_padding_mask is None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argparse.ArgumentParser()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a separate new file hello.py in folder my_files/\n",
    "\n",
    "\n",
    "[link: TowardsDataScience](https://towardsdatascience.com/a-simple-guide-to-command-line-arguments-with-argparse-6824c30ab1c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--name', type=str, required=True)\n",
    "# args = parser.parse_args()\n",
    "# print('Hello,', args.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in command line:\n",
    "\n",
    "1. cd my_files/\n",
    "2. python hello.py --name Ugne\n",
    "\n",
    "\n",
    "output:\n",
    "\n",
    "Hello, Ugne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I kind of tell python to execute file hello.py and also give the required argument --name: \"Ugne\"\n",
    "\n",
    "it executes the file so it checks if I gave the necessary argument and then does what the file tells: print('Hello,', args.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ._ _ dict _ _."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temp': 1}\n"
     ]
    }
   ],
   "source": [
    "def func():\n",
    "    pass\n",
    "\n",
    "func.temp = 1\n",
    "\n",
    "print(func.__dict__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(prior, \n",
    "#     criterion, \n",
    "#     encoder_generator, \n",
    "#     y_encoder_generator=y_encoder_generator, \n",
    "#     pos_encoder_generator=pos_encoder_generator, \n",
    "#     **args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def train(priordataloader_class, \n",
    "          criterion, \n",
    "          encoder_generator, \n",
    "          emsize=200, nhid=200, nlayers=6, nhead=2, dropout=0.0,\n",
    "          epochs=10, steps_per_epoch=100, batch_size=200, bptt=10, lr=None, weight_decay=0.0, warmup_epochs=10, input_normalization=False,\n",
    "          y_encoder_generator=None, pos_encoder_generator=None, decoder=None, extra_prior_kwargs_dict={}, scheduler=get_cosine_schedule_with_warmup,\n",
    "          load_weights_from_this_state_dict=None, validation_period=10, single_eval_pos_gen=None, bptt_extra_samples=None, gpu_device='cuda:0',\n",
    "          aggregate_k_gradients=1, verbose=True, style_encoder_generator=None, epoch_callback=None,\n",
    "          initializer=None, initialize_with_model=None, train_mixed_precision=False, efficient_eval_masking=True, **model_extra_args\n",
    "          ):\n",
    "\n",
    "    def eval_pos_seq_len_sampler():\n",
    "        single_eval_pos = single_eval_pos_gen()\n",
    "        if bptt_extra_samples:\n",
    "            return single_eval_pos, single_eval_pos + bptt_extra_samples\n",
    "        else:\n",
    "            return single_eval_pos, bptt\n",
    "\n",
    "    # haven't found this function priordataloader_class() in other docs - where is it defined?        \n",
    "    dl = priordataloader_class(num_steps=steps_per_epoch, \n",
    "                               batch_size=batch_size, \n",
    "                               eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, \n",
    "                               seq_len_maximum=bptt+(bptt_extra_samples if bptt_extra_samples else 0), \n",
    "                               device=device, \n",
    "                               **extra_prior_kwargs_dict)\n",
    "\n",
    "    encoder = encoder_generator(dl.num_features, emsize)\n",
    "    #style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)\n",
    "    style_def = None\n",
    "    #print(f'Style definition of first 3 examples: {style_def[:3] if style_def is not None else None}')\n",
    "    style_encoder = style_encoder_generator(style_def.shape[1], emsize) if (style_def is not None) else None\n",
    "    if isinstance(criterion, nn.GaussianNLLLoss):\n",
    "        n_out = 2\n",
    "    elif isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        n_out = criterion.weight.shape[0]\n",
    "    else:\n",
    "        n_out = 1\n",
    "\n",
    "    model = TransformerModel(encoder, n_out, emsize, nhead, nhid, nlayers, dropout, style_encoder=style_encoder,\n",
    "                             y_encoder=y_encoder_generator(1, emsize), input_normalization=input_normalization,\n",
    "                             pos_encoder=(pos_encoder_generator or positional_encodings.NoPositionalEncoding)(emsize, bptt*2),\n",
    "                             decoder=decoder, init_method=initializer, efficient_eval_masking=efficient_eval_masking, **model_extra_args\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "style_encoder_generator = None\n",
    "\n",
    "style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)\n",
    "style_def = None\n",
    "#print(f'Style definition of first 3 examples: {style_def[:3] if style_def is not None else None}')\n",
    "style_encoder = style_encoder_generator(style_def.shape[1], emsize) if (style_def is not None) else None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) they import encoders so the content of encoders.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import tabpfn.encoders as encoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in file encoders.py they have this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "Linear = nn.Linear\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, num_features, emsize, replace_nan_by_zero=False):\n",
    "        super().__init__(num_features, emsize)\n",
    "        self.num_features = num_features\n",
    "        self.emsize = emsize\n",
    "        self.replace_nan_by_zero = replace_nan_by_zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.replace_nan_by_zero:\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "        return super().forward(x)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        self.__dict__.setdefault('replace_nan_by_zero', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "MLP = lambda num_features, emsize: nn.Sequential(nn.Linear(num_features+1,emsize*2),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(emsize*2,emsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "class _PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        self.device_test_tensor = nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "    def forward(self, x):# T x B x num_features\n",
    "        assert self.d_model % x.shape[-1]*2 == 0\n",
    "        d_per_feature = self.d_model // x.shape[-1]\n",
    "        pe = torch.zeros(*x.shape, d_per_feature, device=self.device_test_tensor.device)\n",
    "        #position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        interval_size = 10\n",
    "        div_term = (1./interval_size) * 2*math.pi*torch.exp(torch.arange(0, d_per_feature, 2, device=self.device_test_tensor.device).float()*math.log(math.sqrt(2)))\n",
    "        #print(div_term/2/math.pi)\n",
    "        pe[..., 0::2] = torch.sin(x.unsqueeze(-1) * div_term)\n",
    "        pe[..., 1::2] = torch.cos(x.unsqueeze(-1) * div_term)\n",
    "        return self.dropout(pe).view(x.shape[0],x.shape[1],self.d_model)\n",
    "\n",
    "\n",
    "Positional = lambda _, emsize: _PositionalEncoding(d_model=emsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) with this we're able to set the encoder and y_encoder to specific values (done through command line as I understand?):\n",
    "\n",
    "* --encoder 'linear' / 'mlp' / 'positional'\n",
    "* --y_encoder 'linear' / 'mlp' / 'positional'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def _parse_args(config_parser, parser):\n",
    "    # Do we have a config file to parse?\n",
    "    args_config, remaining = config_parser.parse_known_args()\n",
    "    if args_config.config:\n",
    "        with open(args_config.config, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            parser.set_defaults(**cfg)\n",
    "\n",
    "    # The main arg parser parses the rest of the args, the usual\n",
    "    # defaults will have been overridden if config file specified.\n",
    "    args = parser.parse_args(remaining)\n",
    "\n",
    "    # Cache the args as a text string to save them in the output dir later\n",
    "    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "    return args, args_text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config_parser = argparse.ArgumentParser(description='Only used as a first parser for the config file path.')\n",
    "    config_parser.add_argument('--config')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--encoder', default='linear', type=str, help='Specify depending on the prior.')\n",
    "    parser.add_argument('--y_encoder', default='linear', type=str, help='Specify depending on the prior. You should specify this if you do not fuse x and y.')\n",
    "    parser.add_argument('--pos_encoder', default='none', type=str, help='Specify depending on the prior.')\n",
    "\n",
    "    args, _ = _parse_args(config_parser, parser)\n",
    "\n",
    "    if args.nhid is None:\n",
    "        args.nhid = 2*args.emsize\n",
    "\n",
    "    encoder = args.__dict__.pop('encoder') # sets encoder to the value of dictionary element 'encoder'\n",
    "    y_encoder = args.__dict__.pop('y_encoder') # sets y_encoder to the value of dictionary element 'y_encoder'\n",
    "\n",
    "    \n",
    "    def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.Linear\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLP\n",
    "        elif encoder == 'positional':\n",
    "            encoder_generator = encoders.Positional\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "    encoder_generator = get_encoder_generator(encoder)\n",
    "    y_encoder_generator = get_encoder_generator(y_encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding encoders.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class Linear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* encodes (linearly) all datapoint vectors into new vectors of length = emsize\n",
    "* replaces NaN by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "\n",
    "    def __init__(self, num_features, emsize, replace_nan_by_zero=False):\n",
    "        super().__init__(num_features, emsize)\n",
    "        self.num_features = num_features\n",
    "        self.emsize = emsize\n",
    "        self.replace_nan_by_zero = replace_nan_by_zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.replace_nan_by_zero:\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (input) shape: torch.Size([1000, 20])\n",
      "output shape: torch.Size([1000, 512])\n",
      "tensor([[ 0.0499,  0.3734, -0.9258,  ...,  0.1520,  0.6031,  0.5477],\n",
      "        [-0.4581, -0.3672,  0.7149,  ...,  0.6977, -0.3434, -0.8434],\n",
      "        [ 0.1711, -0.6576, -0.3224,  ..., -0.9545, -0.2267,  0.0727],\n",
      "        ...,\n",
      "        [-0.8495,  0.9168, -0.2270,  ...,  0.1044,  0.6164,  1.6906],\n",
      "        [-0.0899, -0.5649,  1.0796,  ...,  0.4228, -0.5281, -0.0314],\n",
      "        [ 0.5822,  0.6002, -0.1890,  ..., -0.3911, -0.4425,  0.2561]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_features = 20 # no. of features - no default, depends on the prior, see train.py\n",
    "em_size = 512 # default, see train.py\n",
    "batch = 1000 # default, see train.py\n",
    "x = torch.randn(batch, num_features) # 128 vecs, each of length 20 (so each datapoint has 20 features)\n",
    "\n",
    "encoder_ln = Linear(num_features, em_size)\n",
    "output_ln = encoder_ln.forward(x)\n",
    "\n",
    "print(\"x (input) shape:\", x.shape)\n",
    "print(\"output shape:\", output_ln.size())\n",
    "print(output_ln)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class StyleEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encodes numerical features (all features at once)\n",
    "\n",
    "* takes a batch of datapoints and looks at all features - so looks at datapoint vectors of length = num_hyperparameters\n",
    "* encodes (linearly) all datapoint vectors into new vectors of length = em_size\n",
    "\n",
    "essentially does the same as class Linear which additionally encodes NaN to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, num_hyperparameters, em_size):\n",
    "        super().__init__()\n",
    "        self.em_size = em_size\n",
    "        self.embedding = nn.Linear(num_hyperparameters, self.em_size)\n",
    "\n",
    "    def forward(self, hyperparameters):  \n",
    "        return self.embedding(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (input) shape: torch.Size([1000, 20])\n",
      "output shape: torch.Size([1000, 512])\n",
      "tensor([[ 0.0499,  0.3734, -0.9258,  ...,  0.1520,  0.6031,  0.5477],\n",
      "        [-0.4581, -0.3672,  0.7149,  ...,  0.6977, -0.3434, -0.8434],\n",
      "        [ 0.1711, -0.6576, -0.3224,  ..., -0.9545, -0.2267,  0.0727],\n",
      "        ...,\n",
      "        [-0.8495,  0.9168, -0.2270,  ...,  0.1044,  0.6164,  1.6906],\n",
      "        [-0.0899, -0.5649,  1.0796,  ...,  0.4228, -0.5281, -0.0314],\n",
      "        [ 0.5822,  0.6002, -0.1890,  ..., -0.3911, -0.4425,  0.2561]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_hyperparameters = 20 # no. of features - no default, depends on the prior, see train.py\n",
    "em_size = 512 # default, see train.py\n",
    "batch = 1000 # default, see train.py\n",
    "hyperparameters = torch.randn(batch, num_hyperparameters) \n",
    "\n",
    "encoder_st = StyleEncoder(num_hyperparameters, em_size)\n",
    "output_st = encoder_st.forward(hyperparameters)\n",
    "\n",
    "print(\"x (input) shape:\", hyperparameters.shape)\n",
    "print(\"output shape:\", output_st.size())\n",
    "print(output_st)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class StyleEmbEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encodes categorical features (one feature at a time)\n",
    "\n",
    "* takes a batch of datapoints and looks at one feature (assert num_hyperparameters == 1)\n",
    "* encodes all possible values of this feature into vectors of length = em_size\n",
    "* note: max number of distinct values that one feature can get is set to num_embeddings=100 (as I understand)\n",
    "\n",
    "pvz: feature f_1: clothing_size={S,M,L}\n",
    "\n",
    "so clothing_size has 3 classes and each class will get its own unique vector of length = em_size\n",
    "\n",
    "we'll have 3 distinct vecs: one for S, one for M and one for L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEmbEncoder(nn.Module):\n",
    "    def __init__(self, num_hyperparameters, em_size, num_embeddings=100):\n",
    "        super().__init__()\n",
    "        assert num_hyperparameters == 1\n",
    "        self.em_size = em_size\n",
    "        self.embedding = nn.Embedding(num_embeddings, self.em_size)\n",
    "\n",
    "    def forward(self, hyperparameters): \n",
    "        return self.embedding(hyperparameters.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [0],\n",
       "        [5],\n",
       "        [9],\n",
       "        [2],\n",
       "        [3],\n",
       "        [0],\n",
       "        [3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 10, (8, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (input) shape: torch.Size([1000, 1])\n",
      "output shape: torch.Size([1000, 512])\n",
      "tensor([[-1.6819,  2.1511, -0.3536,  ..., -0.5102,  1.3916, -0.1969],\n",
      "        [ 0.4755, -0.1734,  0.2474,  ..., -0.7203,  0.9995, -1.2702],\n",
      "        [ 0.2038,  0.9803, -1.2191,  ..., -0.7268,  1.9840, -1.0883],\n",
      "        ...,\n",
      "        [-2.3043,  0.1429, -1.0265,  ...,  0.7998,  0.8934, -0.7810],\n",
      "        [-1.2654,  0.0638,  0.6347,  ...,  0.0913, -0.4807,  0.1304],\n",
      "        [ 0.0595, -1.0513, -1.8355,  ..., -0.0785,  1.0079, -0.2281]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_embeddings = 100 # given in the initialization of the class\n",
    "num_hyperparameters_1 = 1 # no. of features - no default, depends on the prior, see train.py\n",
    "em_size = 512 # default, see train.py\n",
    "batch = 1000 # default, see train.py\n",
    "hyperparameters = torch.randint(0, num_embeddings, (batch, num_hyperparameters_1))\n",
    "\n",
    "encoder_stemb = StyleEmbEncoder(num_hyperparameters_1, em_size)\n",
    "output_stemb = encoder_stemb.forward(hyperparameters)\n",
    "\n",
    "print(\"x (input) shape:\", hyperparameters.shape)\n",
    "print(\"output shape:\", output_stemb.size())\n",
    "print(output_stemb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, so it works like this:\n",
    "\n",
    "StyleEmbEncoder():\n",
    "\n",
    "* num_hyperparameters = 1 - so we take one feature (say datapoint has 5 features, so we take first feature, which is e.g. gender)\n",
    "* this hyperparameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding transformer.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a class `TransformerModel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, n_out, ninp, nhead, nhid, nlayers, dropout=0.0, style_encoder=None, y_encoder=None,\n",
    "                 pos_encoder=None, decoder=None, input_normalization=False, init_method=None, pre_norm=False,\n",
    "                 activation='gelu', recompute_attn=False, num_global_att_tokens=0, full_attention=False,\n",
    "                 all_layers_same_init=False, efficient_eval_masking=True):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        encoder_layer_creator = lambda: TransformerEncoderLayer(ninp, nhead, nhid, dropout, activation=activation,\n",
    "                                                                pre_norm=pre_norm, recompute_attn=recompute_attn)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer_creator(), nlayers)\\\n",
    "            if all_layers_same_init else TransformerEncoderDiffInit(encoder_layer_creator, nlayers)\n",
    "        self.encoder = encoder\n",
    "        self.y_encoder = y_encoder\n",
    "        self.pos_encoder = pos_encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "within this class there's a function `forward()` which uses the argument `encoder` which is an argument for this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def forward(self, src, src_mask=None, single_eval_pos=None):\n",
    "        assert isinstance(src, tuple), 'inputs (src) have to be given as (x,y) or (style,x,y) tuple'\n",
    "\n",
    "        if len(src) == 2: # (x,y) and no style\n",
    "            src = (None,) + src\n",
    "\n",
    "        style_src, x_src, y_src = src\n",
    "        x_src = self.encoder(x_src)\n",
    "        y_src = self.y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "        style_src = self.style_encoder(style_src).unsqueeze(0) if self.style_encoder else \\\n",
    "            torch.tensor([], device=x_src.device)\n",
    "        global_src = torch.tensor([], device=x_src.device) if self.global_att_embeddings is None else \\\n",
    "            self.global_att_embeddings.weight.unsqueeze(1).repeat(1, x_src.shape[1], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "data = torch.rand(10, 32, 512)\n",
    "query, key, value = data, data, data \n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class TransformerEncoderLayer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# full\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``.\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state): # not sure what it does\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False by default and is not changed in model=TransformerModel() in train.py\n",
    "            src_ = self.norm1(src)\n",
    "            #print(\"not run\")\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        if isinstance(src_mask, tuple): # NOT RUN - AssertionError \n",
    "            # global attention setup\n",
    "            assert not self.self_attn.batch_first # AssertionError when batch_first=True: not True = False  --> so batch_first must be False (and it is - default False is not changed in model=TransformerModel() in train.py)\n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            \n",
    "            # I think this is not run as we get AssertionError: default src_key_padding_mask=None is not changed\n",
    "            # so we actually do what's in else (elif also gets AssertionError fot the same reason)\n",
    "            \n",
    "            global_src_mask, trainset_src_mask, valset_src_mask = src_mask\n",
    "\n",
    "            num_global_tokens = global_src_mask.shape[0]\n",
    "            num_train_tokens = trainset_src_mask.shape[0]\n",
    "\n",
    "            global_tokens_src = src_[:num_global_tokens]\n",
    "            train_tokens_src = src_[num_global_tokens:num_global_tokens+num_train_tokens]\n",
    "            global_and_train_tokens_src = src_[:num_global_tokens+num_train_tokens]\n",
    "            eval_tokens_src = src_[num_global_tokens+num_train_tokens:]\n",
    "\n",
    "\n",
    "            attn = partial(checkpoint, self.self_attn) if self.recompute_attn else self.self_attn\n",
    "\n",
    "            global_tokens_src2 = attn(global_tokens_src, global_and_train_tokens_src, global_and_train_tokens_src, None, True, global_src_mask)[0]\n",
    "            train_tokens_src2 = attn(train_tokens_src, global_tokens_src, global_tokens_src, None, True, trainset_src_mask)[0]\n",
    "            eval_tokens_src2 = attn(eval_tokens_src, src_, src_,\n",
    "                                    None, True, valset_src_mask)[0]\n",
    "\n",
    "            src2 = torch.cat([global_tokens_src2, train_tokens_src2, eval_tokens_src2], dim=0)\n",
    "\n",
    "        elif isinstance(src_mask, int): # NOT RUN - AssertionError \n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            single_eval_position = src_mask\n",
    "            src_left = self.self_attn(src_[:single_eval_position], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src_right = self.self_attn(src_[single_eval_position:], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src2 = torch.cat([src_left, src_right], dim=0)\n",
    "        else: # this gets RUN \n",
    "            if self.recompute_attn: # recompute_attn=False by default, and is not changed in model=TransformerModel() in train.py)\n",
    "                src2 = checkpoint(self.self_attn, src_, src_, src_, src_key_padding_mask, True, src_mask)[0]\n",
    "            else: # so we actually do this part\n",
    "                src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                      key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False\n",
    "            src_ = self.norm2(src)\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src_))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm2(src)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576, 0.2793, 0.4031])\n",
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_full = encoder_layer(src)\n",
    "\n",
    "print(src[0,0,0:3])\n",
    "print(out_full.shape)\n",
    "print(out_full[0,0,0:3])\n",
    "# tensor([ 1.2991, -0.8532, -0.0118]) # when I run with full class definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# commented out what's not run\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``.\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    # def __setstate__(self, state): # not sure what it does\n",
    "    #     if 'activation' not in state:\n",
    "    #         state['activation'] = F.relu\n",
    "    #     super().__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False by default and is not changed in model=TransformerModel() in train.py\n",
    "            # src_ = self.norm1(src)\n",
    "            print(\"not run\")\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        \n",
    "        if isinstance(src_mask, tuple): # NOT RUN - AssertionError \n",
    "            # global attention setup\n",
    "            assert not self.self_attn.batch_first # AssertionError when batch_first=True: not True = False  --> so batch_first must be False (and it is - default False is not changed in model=TransformerModel() in train.py)\n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            \n",
    "            # I think this is not run as we get AssertionError: default src_key_padding_mask=None is not changed\n",
    "            # so we actually do what's in else (elif also gets AssertionError fot the same reason)\n",
    "            \n",
    "            # global_src_mask, trainset_src_mask, valset_src_mask = src_mask\n",
    "\n",
    "            # num_global_tokens = global_src_mask.shape[0]\n",
    "            # num_train_tokens = trainset_src_mask.shape[0]\n",
    "\n",
    "            # global_tokens_src = src_[:num_global_tokens]\n",
    "            # train_tokens_src = src_[num_global_tokens:num_global_tokens+num_train_tokens]\n",
    "            # global_and_train_tokens_src = src_[:num_global_tokens+num_train_tokens]\n",
    "            # eval_tokens_src = src_[num_global_tokens+num_train_tokens:]\n",
    "\n",
    "\n",
    "            # attn = partial(checkpoint, self.self_attn) if self.recompute_attn else self.self_attn\n",
    "\n",
    "            # global_tokens_src2 = attn(global_tokens_src, global_and_train_tokens_src, global_and_train_tokens_src, None, True, global_src_mask)[0]\n",
    "            # train_tokens_src2 = attn(train_tokens_src, global_tokens_src, global_tokens_src, None, True, trainset_src_mask)[0]\n",
    "            # eval_tokens_src2 = attn(eval_tokens_src, src_, src_,\n",
    "            #                         None, True, valset_src_mask)[0]\n",
    "\n",
    "            # src2 = torch.cat([global_tokens_src2, train_tokens_src2, eval_tokens_src2], dim=0)\n",
    "        elif isinstance(src_mask, int): # NOT RUN - AssertionError \n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            # single_eval_position = src_mask\n",
    "            # src_left = self.self_attn(src_[:single_eval_position], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            # src_right = self.self_attn(src_[single_eval_position:], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            # src2 = torch.cat([src_left, src_right], dim=0)\n",
    "        else: # this gets RUN \n",
    "            if self.recompute_attn: # recompute_attn=False by default, and is not changed in model=TransformerModel() in train.py)\n",
    "                # src2 = checkpoint(self.self_attn, src_, src_, src_, src_key_padding_mask, True, src_mask)[0]\n",
    "                print(\"not run\")\n",
    "            else: # so we actually do this part\n",
    "                src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                      key_padding_mask=src_key_padding_mask)[0]\n",
    "        \n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False\n",
    "            src_ = self.norm2(src)\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        \n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src_))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm2(src)\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out = encoder_layer(src)\n",
    "\n",
    "print(out.shape)\n",
    "print(out[0,0,0:3])\n",
    "# tensor([ 1.2991, -0.8532, -0.0118]) # when I run with full class definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# commented out what's not run\n",
    "class DelTransformerEncoderLayer(Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout_ch = Dropout(dropout) # dropout -> dropout_ch\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \n",
    "        # multihead attention\n",
    "        src_ = src\n",
    "        src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # feed forward\n",
    "        src_ = src\n",
    "        src2 = self.linear2(self.dropout_ch(self.activation(self.linear1(src_)))) # dropout -> dropout_ch\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "encoder_layer_del = DelTransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_deleted = encoder_layer_del(src)\n",
    "\n",
    "print(out_deleted.shape)\n",
    "print(out_deleted[0,0,0:3]) # tensor([ 1.2991, -0.8532, -0.0118]) # when I run with only what I think it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "#comparison\n",
    "\n",
    "print(out_full.shape)\n",
    "print(out_deleted.shape)\n",
    "print(out_full[0,0,0:3]) # tensor([ 1.2991, -0.8532, -0.0118]) # when I run with full class definition\n",
    "print(out_deleted[0,0,0:3]) # tensor([ 1.2991, -0.8532, -0.0118]) # when I run with only what I think it does"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576, 0.2793, 0.4031])\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512) # 10 batches where each has 32 datapoints so vecs of length = 512\n",
    "\n",
    "print(src[0,0,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "# passed into train() in train.py\n",
    "emsize=512 #yes, same in the paper\n",
    "nhead=4 #yes, same in the paper\n",
    "nhid=2*emsize # #yes, same in the paper: 1024\n",
    "\n",
    "# # def train() in train.py\n",
    "# emsize=200 # function default \n",
    "# nhead=2 # function default\n",
    "# nhid=200 # function default\n",
    "\n",
    "# model = TransformerModel() in train.py\n",
    "emsize = emsize\n",
    "nhead = nhead\n",
    "nhid = nhid\n",
    "\n",
    "# class TransformerModel() in transformer.py\n",
    "ninp = emsize # ninp - number of inputs\n",
    "nhead = nhead\n",
    "nhid = nhid\n",
    "\n",
    "# class TransformerEncoderLayer() in layer.py\n",
    "# d_model = ninp\n",
    "# nhead = nhead\n",
    "# dim_feedforward = nhid\n",
    "d_model = 512\n",
    "nhead = 4\n",
    "dim_feedforward=2048\n",
    "dropout=0.1\n",
    "activation=\"relu\"\n",
    "layer_norm_eps=1e-5\n",
    "batch_first=False\n",
    "pre_norm=False\n",
    "device=None\n",
    "dtype=None\n",
    "recompute_attn=False\n",
    "factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "# def forward() in class TransformerEncoderLayer() in layer.py\n",
    "src_mask = None\n",
    "src_key_padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Feedforward model\n",
    "linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "dropout_changed = nn.Dropout(dropout) # changed variable name because of TypeError: '<' not supported between instances of 'Dropout' and 'int'\n",
    "linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "norm1 =nn. LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "dropout1 = nn.Dropout(dropout)\n",
    "dropout2 = nn.Dropout(dropout) \n",
    "pre_norm = pre_norm\n",
    "recompute_attn = recompute_attn\n",
    "\n",
    "activation = _get_activation_fn(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs)\n",
    "torch.manual_seed(1)\n",
    "src_ = src\n",
    "        \n",
    "src2 = self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                key_padding_mask=src_key_padding_mask)[0]\n",
    "\n",
    "src = src + dropout1(src2)\n",
    "\n",
    "\n",
    "src = norm1(src)\n",
    "\n",
    "\n",
    "src_ = src\n",
    "\n",
    "src2 = linear2(dropout_changed(activation(linear1(src_)))) # changed variable name because of TypeError: '<' not supported between instances of 'Dropout' and 'int'\n",
    "src = src + dropout2(src2)\n",
    "\n",
    "src = norm2(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.3112, -0.7075,  0.6591], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(src.shape)\n",
    "print(src[0,0,0:3])\n",
    "# tensor([-0.4884, -0.7499,  0.4478])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "nhead = 4\n",
    "dim_feedforward=2048\n",
    "dropout=0.1\n",
    "activation=\"relu\"\n",
    "layer_norm_eps=1e-5\n",
    "batch_first=False\n",
    "pre_norm=False\n",
    "device=None\n",
    "dtype=None\n",
    "recompute_attn=False\n",
    "        \n",
    "factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "\n",
    "self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs)\n",
    "\n",
    "# Implementation of Feedforward model\n",
    "linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "dropout_cha = Dropout(dropout)\n",
    "linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "dropout1 = Dropout(dropout)\n",
    "dropout2 = Dropout(dropout)\n",
    "pre_norm = pre_norm\n",
    "recompute_attn = recompute_attn\n",
    "\n",
    "activation = _get_activation_fn(activation)\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "\n",
    "src_mask = None\n",
    "src_key_padding_mask = None\n",
    "\n",
    "src_ = src\n",
    "\n",
    "src2 = self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                key_padding_mask=src_key_padding_mask)[0]\n",
    "\n",
    "src = src + dropout1(src2)\n",
    "\n",
    "src = norm1(src)\n",
    "\n",
    "src_ = src\n",
    "\n",
    "src2 = linear2(dropout_cha(activation(linear1(src_))))\n",
    "src = src + dropout2(src2)\n",
    "\n",
    "src = norm2(src)\n",
    "\n",
    "src_out_trial3 = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 0.7857, -1.1753,  0.0188], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(src_out_trial3.shape)\n",
    "print(src_out_trial3[0,0,0:3]) # tensor([ 0.7857, -1.1753,  0.0188])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globally"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a function `train()` in train.py\n",
    "\n",
    "One of its arguments is encoder_generator - a class (Linear/MLP/Positional from encoders.py)\n",
    "\n",
    "Then in this function train() we create an object of class encoder_generator: `encoder` = encoder_generator(dl.num_features, emsize)\n",
    "\n",
    "pvz.: \n",
    "\n",
    "* encoder_generator = encoders.Linear\n",
    "* encoder = encoders.Linear(num_features=[default 512], emzise=[depends on the prior])\n",
    "\n",
    "With this object encoder we can encode a given dataset x, pvz give 100 datapoints with 20 features in each of them (then emsize=20) and if we do x_encoded=encoder.forward(x) then we get an encoded dataset x_encoded with 100 datapoints where each datapoint now has 512 elements in it\n",
    "\n",
    "Then this function train() sets model = TransformerModel() - an object of class `TransformerModel()` from transformer.py\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a class `TransformerModel()` in transformer.py\n",
    "\n",
    "Within this class there's a function `forward()` which uses arguments `encoder` and `y_encoder` which are the arguments for this class\n",
    "\n",
    "Then funciton forward() basically performs encoding of source datapoints (x,y):\n",
    "\n",
    "* x_src = self.encoder(x_src)\n",
    "* y_src = self.y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
