{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Module, TransformerEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "import gpytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test batch (fast_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_features = 20 # setting this manually\n",
    "\n",
    "class PriorDataLoader(DataLoader):\n",
    "    pass\n",
    "    # init accepts num_steps as first argument\n",
    "\n",
    "    # has two attributes set on class or object level:\n",
    "    # num_features: int and\n",
    "    # num_outputs: int\n",
    "    # fuse_x_y: bool\n",
    "    # Optional: validate function that accepts a transformer model\n",
    "\n",
    "# tabpfn/utils.py\n",
    "def set_locals_in_self(locals):\n",
    "    \"\"\"\n",
    "    Call this function like `set_locals_in_self(locals())` to set all local variables as object variables.\n",
    "    Especially useful right at the beginning of `__init__`.\n",
    "    :param locals: `locals()`\n",
    "    \"\"\"\n",
    "    self = locals['self']\n",
    "    for var_name, val in locals.items():\n",
    "        if var_name != 'self': setattr(self, var_name, val)\n",
    "\n",
    "default_device = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\n",
    "\n",
    "# priors/utils.py\n",
    "def get_batch_to_dataloader(get_batch_method_):\n",
    "    class DL(PriorDataLoader):\n",
    "        get_batch_method = get_batch_method_\n",
    "\n",
    "        num_features = num_features\n",
    "\n",
    "        # Caution, you might need to set self.num_features manually if it is not part of the args.\n",
    "        def __init__(self, num_steps, **get_batch_kwargs):\n",
    "            set_locals_in_self(locals())\n",
    "\n",
    "            # The stuff outside the or is set as class attribute before instantiation.\n",
    "            self.num_features = get_batch_kwargs.get('num_features') or self.num_features\n",
    "            self.epoch_count = 0\n",
    "            #print('DataLoader.__dict__', self.__dict__)\n",
    "\n",
    "        @staticmethod\n",
    "        def gbm(*args, eval_pos_seq_len_sampler, **kwargs):\n",
    "            kwargs['single_eval_pos'], kwargs['seq_len'] = eval_pos_seq_len_sampler()\n",
    "            # Scales the batch size dynamically with the power of 'dynamic_batch_size'.\n",
    "            # A transformer with quadratic memory usage in the seq len would need a power of 2 to keep memory constant.\n",
    "            if 'dynamic_batch_size' in kwargs and kwargs['dynamic_batch_size'] > 0 and kwargs['dynamic_batch_size']:\n",
    "                kwargs['batch_size'] = kwargs['batch_size'] * math.floor(math.pow(kwargs['seq_len_maximum'], kwargs['dynamic_batch_size']) / math.pow(kwargs['seq_len'], kwargs['dynamic_batch_size']))\n",
    "            batch = get_batch_method_(*args, **kwargs)\n",
    "            x, y, target_y, style = batch if len(batch) == 4 else (batch[0], batch[1], batch[2], None)\n",
    "            return (style, x, y), target_y, kwargs['single_eval_pos']\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_steps\n",
    "\n",
    "        def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "        def __iter__(self):\n",
    "            assert hasattr(self, 'model'), \"Please assign model with `dl.model = ...` before training.\"\n",
    "            self.epoch_count += 1\n",
    "            return iter(self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count - 1, model=self.model) for _ in range(self.num_steps))\n",
    "\n",
    "    return DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from priors/fast_gp.py\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def get_model(x, y, hyperparameters):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1.e-9))\n",
    "    model = ExactGPModel(x, y, likelihood)\n",
    "    model.likelihood.noise = torch.ones_like(model.likelihood.noise) * hyperparameters[\"noise\"]\n",
    "    model.covar_module.outputscale = torch.ones_like(model.covar_module.outputscale) * hyperparameters[\"outputscale\"]\n",
    "    model.covar_module.base_kernel.lengthscale = torch.ones_like(model.covar_module.base_kernel.lengthscale) * \\\n",
    "                                                 hyperparameters[\"lengthscale\"]\n",
    "    return model, likelihood\n",
    "\n",
    "# manually setting num_features=20\n",
    "@torch.no_grad()\n",
    "def get_batch(batch_size, seq_len, num_features=20, device=default_device, hyperparameters=None,\n",
    "              equidistant_x=False, fix_x=None, **kwargs):\n",
    "    if isinstance(hyperparameters, (tuple, list)):\n",
    "        hyperparameters = {\"noise\": hyperparameters[0]\n",
    "            , \"outputscale\": hyperparameters[1]\n",
    "            , \"lengthscale\": hyperparameters[2]\n",
    "            , \"is_binary_classification\": hyperparameters[3]\n",
    "            # , \"num_features_used\": hyperparameters[4]\n",
    "            , \"normalize_by_used_features\": hyperparameters[5]\n",
    "            , \"order_y\": hyperparameters[6]\n",
    "            , \"sampling\": hyperparameters[7]\n",
    "                           }\n",
    "    elif hyperparameters is None:\n",
    "        hyperparameters = {\"noise\": .1, \"outputscale\": .1, \"lengthscale\": .1}\n",
    "\n",
    "    if 'verbose' in hyperparameters and hyperparameters['verbose']:\n",
    "        print({\"noise\": hyperparameters['noise'], \"outputscale\": hyperparameters['outputscale']\n",
    "                  , \"lengthscale\": hyperparameters['lengthscale'], 'batch_size': batch_size, 'sampling': hyperparameters['sampling']})\n",
    "\n",
    "    # hyperparameters = {k: hyperparameters[k]() if callable(hyperparameters[k]) else hyperparameters[k] for k in\n",
    "    #      hyperparameters.keys()}\n",
    "    assert not (equidistant_x and (fix_x is not None))\n",
    "\n",
    "    with gpytorch.settings.fast_computations(*hyperparameters.get('fast_computations', (True, True, True))):\n",
    "        if equidistant_x:\n",
    "            assert num_features == 1\n",
    "            x = torch.linspace(0, 1., seq_len).unsqueeze(0).repeat(batch_size, 1).unsqueeze(-1)\n",
    "        elif fix_x is not None:\n",
    "            assert fix_x.shape == (seq_len, num_features)\n",
    "            x = fix_x.unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        else:\n",
    "            if hyperparameters.get('sampling','uniform') == 'uniform':\n",
    "                x = torch.rand(batch_size, seq_len, num_features, device=device)\n",
    "            else:\n",
    "                x = torch.randn(batch_size, seq_len, num_features, device=device)\n",
    "        model, likelihood = get_model(x, torch.Tensor(), hyperparameters)\n",
    "        model.to(device)\n",
    "        # trained_model = ExactGPModel(train_x, train_y, likelihood).cuda()\n",
    "        # trained_model.eval()\n",
    "        is_fitted = False\n",
    "        while not is_fitted:\n",
    "            try:\n",
    "                with gpytorch.settings.prior_mode(True):\n",
    "                    model, likelihood = get_model(x, torch.Tensor(), hyperparameters)\n",
    "                    model.to(device)\n",
    "\n",
    "                    d = model(x)\n",
    "                    d = likelihood(d)\n",
    "                    sample = d.sample().transpose(0, 1)\n",
    "                    is_fitted = True\n",
    "            except RuntimeError: # This can happen when torch.linalg.eigh fails. Restart with new init resolves this.\n",
    "                print('GP Fitting unsuccessful, retrying.. ')\n",
    "                print(x)\n",
    "                print(hyperparameters)\n",
    "\n",
    "    if bool(torch.any(torch.isnan(x)).detach().cpu().numpy()):\n",
    "        print({\"noise\": hyperparameters['noise'], \"outputscale\": hyperparameters['outputscale']\n",
    "                  , \"lengthscale\": hyperparameters['lengthscale'], 'batch_size': batch_size})\n",
    "\n",
    "    # TODO: Multi output\n",
    "    return x.transpose(0, 1), sample, sample  # x.shape = (T,B,H)\n",
    "\n",
    "DataLoader = get_batch_to_dataloader(get_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabpfn/train.py\n",
    "torch.manual_seed(1)\n",
    "steps_per_epoch = 100 # set to 10\n",
    "batch_size = 200 # set to 1000\n",
    "bptt=10 # default in function train(), not changed afterwards\n",
    "bptt_extra_samples=None # default in function train(), not changed afterwards\n",
    "#single_eval_pos_gen=None # default in function train(), not changed afterwards\n",
    "extra_prior_kwargs_dict={} # default in function train(), not changed afterwards\n",
    "gpu_device='cuda:0' # default in function train(), not changed afterwards\n",
    "device = gpu_device if torch.cuda.is_available() else 'cpu:0'\n",
    "\n",
    "def eval_pos_seq_len_sampler():\n",
    "    single_eval_pos = single_eval_pos_gen()\n",
    "    if bptt_extra_samples:\n",
    "        return single_eval_pos, single_eval_pos + bptt_extra_samples\n",
    "    else:\n",
    "        return single_eval_pos, bptt\n",
    "\n",
    "priordataloader_class = DataLoader\n",
    "\n",
    "dl = priordataloader_class(num_steps=steps_per_epoch, batch_size=batch_size, eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, seq_len_maximum=bptt+(bptt_extra_samples if bptt_extra_samples else 0), device=device, **extra_prior_kwargs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "def get_uniform_single_eval_pos_sampler(max_len, min_len=0):\n",
    "    \"\"\"\n",
    "    Just sample any evaluation position with the same weight\n",
    "    :return: Sampler that can be fed to `train()` as `single_eval_pos_gen`.\n",
    "    \"\"\"\n",
    "    return lambda: random.choices(range(min_len, max_len))[0]\n",
    "\n",
    "get_sampler = get_uniform_single_eval_pos_sampler\n",
    "permutation_invariant_max_eval_pos = 100 # very random, had to set it to sth but don't know what this is\n",
    "\n",
    "single_eval_pos_gen = get_sampler(permutation_invariant_max_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gp = dl.get_test_batch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([10, 200, 20])\n",
      "torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "# tuple: (style, x, y)\n",
    "print(dl.get_test_batch()[0][0]) # style: seems like it's None :(\n",
    "print(dl.get_test_batch()[0][1].shape) # x: seems like its a batch of 10 samples where each has 200 x vectors each with 20 features\n",
    "print(dl.get_test_batch()[0][2].shape) # y: seems like its a batch of 10 samples where each has 200 x vectors of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7474, 0.6250, 0.1107, 0.2828, 0.1912],\n",
      "        [0.7711, 0.6751, 0.0138, 0.4008, 0.6349],\n",
      "        [0.5934, 0.3755, 0.3774, 0.0090, 0.6477],\n",
      "        [0.1862, 0.3648, 0.1937, 0.8451, 0.6535]])\n",
      "tensor([ 0.9026,  0.1836, -0.0214,  0.3186])\n"
     ]
    }
   ],
   "source": [
    "print(dl.get_test_batch()[0][1][0,0:4,0:5]) # rows are vecs x1, x2, x3, x4\n",
    "print(dl.get_test_batch()[0][2][0,0:4]) # elements are values y1, y2, y3, y4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_gp #src is a tuple of the form: ((style, x, y), target, single_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "None\n",
      "torch.Size([10, 200, 20])\n",
      "torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "# (style, x, y)\n",
    "print(len(data[0]))\n",
    "print(data[0][0])\n",
    "print(data[0][1].shape)\n",
    "print(data[0][2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200])\n",
      "torch.Size([200])\n",
      "torch.Size([200])\n",
      "torch.Size([200])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# target (target y)\n",
    "print(data[1].shape)\n",
    "print(data[1][0].shape)\n",
    "print(data[1][1].shape)\n",
    "print(data[1][2].shape)\n",
    "# ...\n",
    "print(data[1][9].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# single_eval_pos\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "tensor([-0.2304, -0.4354, -0.2079,  0.2077, -0.3644,  0.0663])\n",
      "tensor([-0.2304, -0.4354, -0.2079,  0.2077, -0.3644,  0.0663])\n"
     ]
    }
   ],
   "source": [
    "# print(src[1])\n",
    "# print(src[0][2])\n",
    "\n",
    "print(data[1].shape) # so we have 10 batches where each has 200 datapoints\n",
    "print(data[0][2].shape) # so we have 10 batches where each has 200 datapoints\n",
    "\n",
    "print(data[1][9][0:6]) # here we see first 5 datapoints from 10th batch\n",
    "print(data[0][2][9][0:6]) # here we see first 5 datapoints from 10th batch\n",
    "\n",
    "# tensor([-0.2304, -0.4354, -0.2079,  0.2077, -0.3644,  0.0663])\n",
    "# tensor([-0.2304, -0.4354, -0.2079,  0.2077, -0.3644,  0.0663])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerModel() - manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passed into train() in train.py\n",
    "emsize=512 #yes, same in the paper\n",
    "nhead=4 #yes, same in the paper\n",
    "nhid=2*emsize # #yes, same in the paper: 1024\n",
    "# nlayers=6 # hmm, paper says 12\n",
    "nlayers=1\n",
    "\n",
    "# encoder = [defined later]\n",
    "n_out = 1 # can be 2 or sth else\n",
    "ninp = emsize\n",
    "nhead = nhead\n",
    "nhid = 2*emsize\n",
    "nlayers = nlayers\n",
    "dropout=0.0\n",
    "style_encoder=None\n",
    "y_encoder=None\n",
    "pos_encoder=None\n",
    "decoder=None\n",
    "input_normalization=False\n",
    "init_method=None\n",
    "pre_norm=False\n",
    "activation='gelu'\n",
    "recompute_attn=False\n",
    "num_global_att_tokens=0\n",
    "full_attention=False\n",
    "all_layers_same_init=False\n",
    "efficient_eval_masking=True\n",
    "\n",
    "num_features = 20 # no default, depends on the dataset\n",
    "\n",
    "# defined later:\n",
    "global_att_embeddings = None # because num_global_att_tokens=0\n",
    "input_ln = None\n",
    "decoder = nn.Sequential(nn.Linear(ninp, nhid), nn.GELU(), nn.Linear(nhid, n_out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerEncoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# full\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``.\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state): # not sure what it does\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        torch.manual_seed(1)\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False by default and is not changed in model=TransformerModel() in train.py\n",
    "            src_ = self.norm1(src)\n",
    "            #print(\"not run\")\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        if isinstance(src_mask, tuple): # NOT RUN - AssertionError \n",
    "            # global attention setup\n",
    "            assert not self.self_attn.batch_first # AssertionError when batch_first=True: not True = False  --> so batch_first must be False (and it is - default False is not changed in model=TransformerModel() in train.py)\n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            \n",
    "            # I think this is not run as we get AssertionError: default src_key_padding_mask=None is not changed\n",
    "            # so we actually do what's in else (elif also gets AssertionError fot the same reason)\n",
    "            \n",
    "            global_src_mask, trainset_src_mask, valset_src_mask = src_mask\n",
    "\n",
    "            num_global_tokens = global_src_mask.shape[0]\n",
    "            num_train_tokens = trainset_src_mask.shape[0]\n",
    "\n",
    "            global_tokens_src = src_[:num_global_tokens]\n",
    "            train_tokens_src = src_[num_global_tokens:num_global_tokens+num_train_tokens]\n",
    "            global_and_train_tokens_src = src_[:num_global_tokens+num_train_tokens]\n",
    "            eval_tokens_src = src_[num_global_tokens+num_train_tokens:]\n",
    "\n",
    "\n",
    "            attn = partial(checkpoint, self.self_attn) if self.recompute_attn else self.self_attn\n",
    "\n",
    "            global_tokens_src2 = attn(global_tokens_src, global_and_train_tokens_src, global_and_train_tokens_src, None, True, global_src_mask)[0]\n",
    "            train_tokens_src2 = attn(train_tokens_src, global_tokens_src, global_tokens_src, None, True, trainset_src_mask)[0]\n",
    "            eval_tokens_src2 = attn(eval_tokens_src, src_, src_,\n",
    "                                    None, True, valset_src_mask)[0]\n",
    "\n",
    "            src2 = torch.cat([global_tokens_src2, train_tokens_src2, eval_tokens_src2], dim=0)\n",
    "\n",
    "        elif isinstance(src_mask, int): # NOT RUN - AssertionError \n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            single_eval_position = src_mask\n",
    "            src_left = self.self_attn(src_[:single_eval_position], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src_right = self.self_attn(src_[single_eval_position:], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src2 = torch.cat([src_left, src_right], dim=0)\n",
    "        else: # this gets RUN \n",
    "            if self.recompute_attn: # recompute_attn=False by default, and is not changed in model=TransformerModel() in train.py)\n",
    "                src2 = checkpoint(self.self_attn, src_, src_, src_, src_key_padding_mask, True, src_mask)[0]\n",
    "            else: # so we actually do this part\n",
    "                src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                      key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False\n",
    "            src_ = self.norm2(src)\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src_))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm2(src)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576, 0.2793, 0.4031])\n",
      "torch.Size([10, 32, 512])\n",
      "tensor([ 0.5695, -1.0787,  0.1266], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = src = torch.rand(10, 32, 512)\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_full = encoder_layer(src)\n",
    "\n",
    "print(src[0,0,0:3])\n",
    "print(out_full.shape)\n",
    "print(out_full[0,0,0:3]) # tensor([ 0.5695, -1.0787,  0.1266])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DelTransformerEncoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# commented out what's not run\n",
    "class DelTransformerEncoderLayer(Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout_ch = Dropout(dropout) # dropout -> dropout_ch\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        torch.manual_seed(1)\n",
    "        \n",
    "        # multihead attention\n",
    "        src_ = src\n",
    "        src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # feed forward\n",
    "        src_ = src\n",
    "        src2 = self.linear2(self.dropout_ch(self.activation(self.linear1(src_)))) # dropout -> dropout_ch\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 0.5695, -1.0787,  0.1266], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = src = torch.rand(10, 32, 512)\n",
    "\n",
    "encoder_layer_del = DelTransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_deleted = encoder_layer_del(src)\n",
    "\n",
    "print(out_deleted.shape)\n",
    "print(out_deleted[0,0,0:3]) # tensor([ 0.5695, -1.0787,  0.1266])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_generator = Linear\n",
    "y_encoder_generator = Linear\n",
    "style_encoder_generator = None\n",
    "\n",
    "\n",
    "encoder = encoder_generator(num_features, emsize)\n",
    "y_encoder = y_encoder_generator(1, emsize)\n",
    "style_encoder = style_encoder_generator(style_def.shape[1], emsize) if (style_def is not None) else None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodr block - with 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer_creator = lambda: TransformerEncoderLayer(ninp, nhead, nhid, dropout, activation=activation,\n",
    "                                                                pre_norm=pre_norm, recompute_attn=recompute_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabpfn/utils.py\n",
    "def bool_mask_to_att_mask(mask):\n",
    "    return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabpfn/transformer.py\n",
    "\n",
    "# @staticmethod\n",
    "def generate_D_q_matrix(sz, query_size):\n",
    "    \"\"\"Generates same attnetion matrix as in paper (first one\n",
    "    with the diagonal being one) except all 1 entries are 0.0 \n",
    "    and 0 entries are -inf\n",
    "\n",
    "    Args:\n",
    "        sz (int): batch size\n",
    "        query_size (int): number of query\n",
    "\n",
    "    Returns:\n",
    "        tensor: mask that masks y but attends itself (diagonal 0.0 NOT -inf)\n",
    "    \"\"\"\n",
    "    train_size = sz-query_size\n",
    "    mask = torch.zeros(sz,sz) == 0\n",
    "    mask[:,train_size:].zero_()\n",
    "    mask |= torch.eye(sz) == 1\n",
    "    return bool_mask_to_att_mask(mask)\n",
    "\n",
    "class TransformerEncoderDiffInit(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer_creator: a function generating objects of TransformerEncoderLayer class without args (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer_creator, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer_creator() for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = TransformerEncoderDiffInit(encoder_layer_creator, nlayers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(self):\n",
    "    initrange = 1.\n",
    "    # if isinstance(self.encoder,EmbeddingEncoder):\n",
    "    #    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "    # self.decoder.bias.data.zero_()\n",
    "    # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    if self.init_method is not None:\n",
    "        self.apply(self.init_method)\n",
    "    for layer in self.transformer_encoder.layers:\n",
    "        nn.init.zeros_(layer.linear2.weight)\n",
    "        nn.init.zeros_(layer.linear2.bias)\n",
    "        attns = layer.self_attn if isinstance(layer.self_attn, nn.ModuleList) else [layer.self_attn]\n",
    "        for attn in attns:\n",
    "            nn.init.zeros_(attn.out_proj.weight)\n",
    "            nn.init.zeros_(attn.out_proj.bias)\n",
    "\n",
    "def forward(self, src, src_mask=None, single_eval_pos=None):\n",
    "    # (1)\n",
    "    assert isinstance(src, tuple), 'inputs (src) have to be given as (x,y) or (style,x,y) tuple'\n",
    "\n",
    "    # (2)\n",
    "    if len(src) == 2: # (x,y) and no style\n",
    "        src = (None,) + src\n",
    "\n",
    "    # (3)\n",
    "    style_src, x_src, y_src = src\n",
    "    # (4)\n",
    "    x_src = self.encoder(x_src)\n",
    "    y_src = self.y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "    style_src = self.style_encoder(style_src).unsqueeze(0) if self.style_encoder else \\\n",
    "        torch.tensor([], device=x_src.device)\n",
    "    global_src = torch.tensor([], device=x_src.device) if self.global_att_embeddings is None else \\\n",
    "        self.global_att_embeddings.weight.unsqueeze(1).repeat(1, x_src.shape[1], 1)\n",
    "\n",
    "    if src_mask is not None: assert self.global_att_embeddings is None or isinstance(src_mask, tuple)\n",
    "    if src_mask is None: # this is RUN: default src_mask=None not changed it seems\n",
    "        if self.global_att_embeddings is None: # this is RUN: global_att_embeddings=None it seems\n",
    "            # (5)\n",
    "            full_len = len(x_src) + len(style_src)\n",
    "            if self.full_attention: # NOT RUN: full_attention=False\n",
    "                src_mask = bool_mask_to_att_mask(torch.ones((full_len, full_len), dtype=torch.bool)).to(x_src.device)\n",
    "            elif self.efficient_eval_masking: # this is RUN: efficient_eval_masking=True\n",
    "                src_mask = single_eval_pos + len(style_src)\n",
    "            else:\n",
    "                src_mask = self.generate_D_q_matrix(full_len, len(x_src) - single_eval_pos).to(x_src.device)\n",
    "        else:\n",
    "            src_mask_args = (self.global_att_embeddings.num_embeddings,\n",
    "                                len(x_src) + len(style_src),\n",
    "                                len(x_src) + len(style_src) - single_eval_pos)\n",
    "            src_mask = (self.generate_global_att_globaltokens_matrix(*src_mask_args).to(x_src.device),\n",
    "                        self.generate_global_att_trainset_matrix(*src_mask_args).to(x_src.device),\n",
    "                        self.generate_global_att_query_matrix(*src_mask_args).to(x_src.device))\n",
    "    # (6)\n",
    "    train_x = x_src[:single_eval_pos] + y_src[:single_eval_pos]\n",
    "    # (7)\n",
    "    src = torch.cat([global_src, style_src, train_x, x_src[single_eval_pos:]], 0)\n",
    "\n",
    "    if self.input_ln is not None: # NOT RUN: input_ln=None\n",
    "        src = self.input_ln(src)\n",
    "\n",
    "    if self.pos_encoder is not None: # NOT RUN: pos_encoder=None\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "    # (8)\n",
    "    output = self.transformer_encoder(src, src_mask)\n",
    "    # (9)\n",
    "    output = self.decoder(output)\n",
    "    # (10)\n",
    "    return output[single_eval_pos+len(style_src)+(self.global_att_embeddings.num_embeddings if self.global_att_embeddings else 0):]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward() - step by step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "3\n",
      "None\n",
      "torch.Size([10, 200, 20])\n",
      "torch.Size([10, 200])\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "# (0)\n",
    "# data = ((style, x, y), target, single_eval_pos)\n",
    "src = data[0] # (style, x, y)\n",
    "target = data[1]\n",
    "single_eval_pos = data[2] \n",
    "\n",
    "print(type(src))\n",
    "print(len(src))\n",
    "print(src[0]) # style\n",
    "print(src[1].shape) # x\n",
    "print(src[2].shape) # y\n",
    "print(single_eval_pos) # single_eval_pos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1)\n",
    "isinstance(src, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# (2)\n",
    "print(len(src))\n",
    "print(len(src) == 2)\n",
    "\n",
    "if len(src) == 2: # (x,y) and no style\n",
    "    src = (None,) + src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([2000, 20])\n",
      "torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# (3)\n",
    "# option 1 (default I think):\n",
    "# style_src, x_src, y_src = src\n",
    "\n",
    "# option 2 (how I think it should be):\n",
    "style_src, x_src, y_src = src\n",
    "x_src = x_src.flatten(0,1)\n",
    "y_src = y_src.flatten(0,1)\n",
    "\n",
    "# option 3:\n",
    "# style_src, x_src, y_src = src\n",
    "# x_src = torch.unsqueeze(x_src.flatten(0,1), dim=0)\n",
    "# y_src = torch.unsqueeze(y_src.flatten(0,1), dim=0)\n",
    "\n",
    "print(style_src)\n",
    "print(x_src.shape)\n",
    "print(y_src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=20, out_features=512, bias=True)\n",
      "Linear(in_features=1, out_features=512, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# y_encoder_generator = Linear\n",
    "# y_encoder = y_encoder_generator(1, emsize)\n",
    "print(encoder)\n",
    "print(y_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "True\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(y_src.shape))\n",
    "print(len(x_src.shape))\n",
    "print(len(y_src.shape) < len(x_src.shape))\n",
    "print(y_src.shape)\n",
    "print(y_src.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 512])\n",
      "torch.Size([2000, 512])\n"
     ]
    }
   ],
   "source": [
    "# (4)\n",
    "x_src = encoder(x_src)\n",
    "y_src = y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "style_src = style_encoder(style_src).unsqueeze(0) if style_encoder else torch.tensor([], device=x_src.device)\n",
    "global_src = torch.tensor([], device=x_src.device) if global_att_embeddings is None else global_att_embeddings.weight.unsqueeze(1).repeat(1, x_src.shape[1], 1)\n",
    "\n",
    "print(x_src.shape)\n",
    "print(y_src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "# (5)\n",
    "full_len = len(x_src) + len(style_src)\n",
    "\n",
    "if full_attention: # NOT RUN: full_attention=False\n",
    "    src_mask = bool_mask_to_att_mask(torch.ones((full_len, full_len), dtype=torch.bool)).to(x_src.device)\n",
    "elif efficient_eval_masking: # this is RUN: efficient_eval_masking=True\n",
    "    src_mask = single_eval_pos + len(style_src)\n",
    "else: # NOT RUN\n",
    "    src_mask = generate_D_q_matrix(full_len, len(x_src) - single_eval_pos).to(x_src.device)\n",
    "\n",
    "print(full_len)\n",
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 512])\n"
     ]
    }
   ],
   "source": [
    "# (6)\n",
    "train_x = x_src[:single_eval_pos] + y_src[:single_eval_pos]\n",
    "\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "torch.Size([2000, 512])\n",
      "torch.Size([2000, 512])\n",
      "torch.Size([71, 512])\n",
      "torch.Size([71, 512])\n"
     ]
    }
   ],
   "source": [
    "print(single_eval_pos)\n",
    "print(x_src.shape)\n",
    "print(y_src.shape)\n",
    "print(x_src[:single_eval_pos].shape)\n",
    "print(y_src[:single_eval_pos].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7)\n",
    "src = torch.cat([global_src, style_src, train_x, x_src[single_eval_pos:]], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n",
      "tensor([])\n",
      "torch.Size([71, 512])\n",
      "torch.Size([1929, 512])\n",
      "torch.Size([2000, 512])\n"
     ]
    }
   ],
   "source": [
    "print(global_src)\n",
    "print(style_src)\n",
    "print(train_x.shape) # first 83 positions of x_src and y_src, added on top of one another\n",
    "print(x_src[single_eval_pos:].shape) # remaining 1917 positions of x_src\n",
    "print(src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-52f10d8c5a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# defned above: transformer_encoder = TransformerEncoderDiffInit(encoder_layer_creator, nlayers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ff8fc26a09ac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d45622cbc2c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0msingle_eval_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0msrc_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0msrc_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4947\u001b[0m     \u001b[0;31m# set up shape vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4948\u001b[0;31m     \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4949\u001b[0m     \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4950\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# (8)\n",
    "# defned above: transformer_encoder = TransformerEncoderDiffInit(encoder_layer_creator, nlayers)\n",
    "output = transformer_encoder(src, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6c112155436c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output = decoder(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-79b807e8418c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_eval_pos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_att_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mglobal_att_embeddings\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# (10)\n",
    "output[single_eval_pos+len(style_src)+(global_att_embeddings.num_embeddings if global_att_embeddings else 0):]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoderDiffInit(\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.0, inplace=False)\n",
      "      (dropout2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2000, 512])\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "print(src.shape)\n",
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-606-0c4e8dafad6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-408-d45622cbc2c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0msingle_eval_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0msrc_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0msrc_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DL_project/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4947\u001b[0m     \u001b[0;31m# set up shape vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4948\u001b[0;31m     \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4949\u001b[0m     \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4950\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# (8) - manually\n",
    "num_layers_test = 1\n",
    "layer1 = nn.ModuleList([encoder_layer_creator() for _ in range(num_layers_test)])\n",
    "\n",
    "output = src\n",
    "for mod in layer1:\n",
    "    output = mod(output, src_mask=src_mask, src_key_padding_mask=None)\n",
    "\n",
    "print(layer1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_test_1 = torch.rand(10,200,512)\n",
    "src_test_2 = torch.rand(1,200,512)\n",
    "src_test_3 = torch.rand(200,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_t1 = transformer_encoder(src_test_1, src_mask)\n",
    "output_t2 = transformer_encoder(src_test_2, src_mask)\n",
    "#output_t3 = transformer_encoder(src_test_3, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 200, 512), grad_fn=<SliceBackward>)\n",
      "tensor([], size=(0, 200, 512), grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output_t1[single_eval_pos+len(style_src)+(global_att_embeddings.num_embeddings if global_att_embeddings else 0):])\n",
    "print(output_t2[single_eval_pos+len(style_src)+(global_att_embeddings.num_embeddings if global_att_embeddings else 0):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_eval_pos+len(style_src)+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200, 512])\n",
      "torch.Size([3, 200, 512])\n",
      "torch.Size([0, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output_t1.shape)\n",
    "print(output_t1[7:].shape)\n",
    "print(output_t1[83:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 200, 512), grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t1[83:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200, 20])\n",
      "torch.Size([2000, 20])\n",
      "torch.Size([1, 2000, 20])\n"
     ]
    }
   ],
   "source": [
    "test_dat = torch.rand(10,200,20)\n",
    "test_dat_flat = test_dat.flatten(0,1)\n",
    "test_dat_back = torch.unsqueeze(test_dat.flatten(0,1), dim=0)\n",
    "print(test_dat.shape)\n",
    "print(test_dat_flat.shape)\n",
    "print(test_dat_back.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 1])"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4, 5)\n",
    "x = torch.unsqueeze(x, dim=-1)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, n_out, ninp, nhead, nhid, nlayers, dropout=0.0, style_encoder=None, y_encoder=None,\n",
    "                 pos_encoder=None, decoder=None, input_normalization=False, init_method=None, pre_norm=False,\n",
    "                 activation='gelu', recompute_attn=False, num_global_att_tokens=0, full_attention=False,\n",
    "                 all_layers_same_init=False, efficient_eval_masking=True):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        encoder_layer_creator = lambda: TransformerEncoderLayer(ninp, nhead, nhid, dropout, activation=activation,\n",
    "                                                                pre_norm=pre_norm, recompute_attn=recompute_attn)\n",
    "        \n",
    "        # Initiate n subsequent layers of transformer (initiated all the same or not)\n",
    "        # all_layers_same_init=False by default and not changed later so we do TransformerEncoderDiffInit(encoder_layer_creator, nlayers)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer_creator(), nlayers)\\\n",
    "            if all_layers_same_init else TransformerEncoderDiffInit(encoder_layer_creator, nlayers)\n",
    "        self.ninp = ninp\n",
    "        \n",
    "        # Store the encoder, decoder modules\n",
    "        self.encoder = encoder\n",
    "        self.y_encoder = y_encoder\n",
    "        self.pos_encoder = pos_encoder\n",
    "        self.decoder = decoder(ninp, nhid, n_out) if decoder is not None else nn.Sequential(nn.Linear(ninp, nhid), nn.GELU(), nn.Linear(nhid, n_out))\n",
    "        self.input_ln = SeqBN(ninp) if input_normalization else None\n",
    "        self.style_encoder = style_encoder\n",
    "        self.init_method = init_method\n",
    "        if num_global_att_tokens is not None: \n",
    "            assert not full_attention\n",
    "        \n",
    "        self.global_att_embeddings = nn.Embedding(num_global_att_tokens, ninp) if num_global_att_tokens else None # seems like global_att_embeddings=None\n",
    "        self.full_attention = full_attention\n",
    "        self.efficient_eval_masking = efficient_eval_masking\n",
    "\n",
    "        self.n_out = n_out\n",
    "        self.nhid = nhid\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        self.__dict__.setdefault('efficient_eval_masking', False)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz):\n",
    "        \"\"\"Generates an upper triangular matrix with -inf and 0.0\n",
    "\n",
    "        Args:\n",
    "            sz (int): Batch size\n",
    "\n",
    "        Returns:\n",
    "            tensor: mask - upper triangular matrix\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        return bool_mask_to_att_mask(mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_D_q_matrix(sz, query_size):\n",
    "        \"\"\"Generates same attnetion matrix as in paper (first one\n",
    "        with the diagonal being one) except all 1 entries are 0.0 \n",
    "        and 0 entries are -inf\n",
    "\n",
    "        Args:\n",
    "            sz (int): batch size\n",
    "            query_size (int): number of query\n",
    "\n",
    "        Returns:\n",
    "            tensor: mask that masks y but attends itself (diagonal 0.0 NOT -inf)\n",
    "        \"\"\"\n",
    "        train_size = sz-query_size\n",
    "        mask = torch.zeros(sz,sz) == 0\n",
    "        mask[:,train_size:].zero_()\n",
    "        mask |= torch.eye(sz) == 1\n",
    "        return bool_mask_to_att_mask(mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_global_att_query_matrix(num_global_att_tokens, seq_len, num_query_tokens):\n",
    "        \"\"\"Generates matrix with row for each query explaining which points it should attend. Includes itself.\n",
    "\n",
    "        Args:\n",
    "            num_global_att_tokens (int): \n",
    "            seq_len (int): number of points in batch (I believe)\n",
    "            num_query_tokens (int): \n",
    "\n",
    "        Returns:\n",
    "            mask: num_query_tokens x (seq_len + num_global_att_tokens - num_query_tokens) \n",
    "    \n",
    "        \"\"\"\n",
    "        train_size = seq_len + num_global_att_tokens - num_query_tokens\n",
    "        sz = seq_len + num_global_att_tokens\n",
    "        mask = torch.zeros(num_query_tokens, sz) == 0\n",
    "        mask[:,train_size:].zero_()\n",
    "        mask[:,train_size:] |= torch.eye(num_query_tokens) == 1\n",
    "        return bool_mask_to_att_mask(mask)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_global_att_trainset_matrix(num_global_att_tokens, seq_len, num_query_tokens):\n",
    "        r\"\"\"Directs attention between the trainset: essentially fully connected\n",
    "\n",
    "        Args:\n",
    "            num_global_att_tokens (int): \n",
    "            seq_len (int): \n",
    "            num_query_tokens (int): \n",
    "\n",
    "        Returns:\n",
    "            tensor: (seq_len + num_global_att_tokens - num_query_tokens) x num_global_tokens\n",
    "        \"\"\"\n",
    "        train_size = seq_len + num_global_att_tokens - num_query_tokens\n",
    "        trainset_size = seq_len - num_query_tokens\n",
    "        mask = torch.zeros(trainset_size, num_global_att_tokens) == 0\n",
    "        #mask[:,num_global_att_tokens:].zero_()\n",
    "        #mask[:,num_global_att_tokens:] |= torch.eye(trainset_size) == 1\n",
    "        return bool_mask_to_att_mask(mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_global_att_globaltokens_matrix(num_global_att_tokens, seq_len, num_query_tokens):\n",
    "        mask = torch.zeros(num_global_att_tokens, num_global_att_tokens+seq_len-num_query_tokens) == 0\n",
    "        return bool_mask_to_att_mask(mask)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 1.\n",
    "        # if isinstance(self.encoder,EmbeddingEncoder):\n",
    "        #    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.zero_()\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        if self.init_method is not None:\n",
    "            self.apply(self.init_method)\n",
    "        for layer in self.transformer_encoder.layers:\n",
    "            nn.init.zeros_(layer.linear2.weight)\n",
    "            nn.init.zeros_(layer.linear2.bias)\n",
    "            attns = layer.self_attn if isinstance(layer.self_attn, nn.ModuleList) else [layer.self_attn]\n",
    "            for attn in attns:\n",
    "                nn.init.zeros_(attn.out_proj.weight)\n",
    "                nn.init.zeros_(attn.out_proj.bias)\n",
    "\n",
    "    def forward(self, src, src_mask=None, single_eval_pos=None):\n",
    "        assert isinstance(src, tuple), 'inputs (src) have to be given as (x,y) or (style,x,y) tuple'\n",
    "\n",
    "        if len(src) == 2: # (x,y) and no style\n",
    "            src = (None,) + src\n",
    "\n",
    "        style_src, x_src, y_src = src\n",
    "        x_src = self.encoder(x_src)\n",
    "        y_src = self.y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "        style_src = self.style_encoder(style_src).unsqueeze(0) if self.style_encoder else \\\n",
    "            torch.tensor([], device=x_src.device)\n",
    "        global_src = torch.tensor([], device=x_src.device) if self.global_att_embeddings is None else \\\n",
    "            self.global_att_embeddings.weight.unsqueeze(1).repeat(1, x_src.shape[1], 1)\n",
    "\n",
    "        if src_mask is not None: assert self.global_att_embeddings is None or isinstance(src_mask, tuple)\n",
    "        if src_mask is None: # this is RUN: default src_mask=None not changed it seems\n",
    "            if self.global_att_embeddings is None: # this is RUN: global_att_embeddings=None it seems\n",
    "                full_len = len(x_src) + len(style_src)\n",
    "                if self.full_attention:\n",
    "                    src_mask = bool_mask_to_att_mask(torch.ones((full_len, full_len), dtype=torch.bool)).to(x_src.device)\n",
    "                elif self.efficient_eval_masking:\n",
    "                    src_mask = single_eval_pos + len(style_src)\n",
    "                else:\n",
    "                    src_mask = self.generate_D_q_matrix(full_len, len(x_src) - single_eval_pos).to(x_src.device)\n",
    "            else:\n",
    "                src_mask_args = (self.global_att_embeddings.num_embeddings,\n",
    "                                 len(x_src) + len(style_src),\n",
    "                                 len(x_src) + len(style_src) - single_eval_pos)\n",
    "                src_mask = (self.generate_global_att_globaltokens_matrix(*src_mask_args).to(x_src.device),\n",
    "                            self.generate_global_att_trainset_matrix(*src_mask_args).to(x_src.device),\n",
    "                            self.generate_global_att_query_matrix(*src_mask_args).to(x_src.device))\n",
    "\n",
    "        train_x = x_src[:single_eval_pos] + y_src[:single_eval_pos]\n",
    "        src = torch.cat([global_src, style_src, train_x, x_src[single_eval_pos:]], 0)\n",
    "\n",
    "        if self.input_ln is not None:\n",
    "            src = self.input_ln(src)\n",
    "\n",
    "        if self.pos_encoder is not None:\n",
    "            src = self.pos_encoder(src)\n",
    "\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output[single_eval_pos+len(style_src)+(self.global_att_embeddings.num_embeddings if self.global_att_embeddings else 0):]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_from_small_model(self, small_model):\n",
    "        assert isinstance(self.decoder, nn.Linear) and isinstance(self.encoder, (nn.Linear, nn.Sequential)) \\\n",
    "               and isinstance(self.y_encoder, (nn.Linear, nn.Sequential))\n",
    "\n",
    "        def set_encoder_weights(my_encoder, small_model_encoder):\n",
    "            my_encoder_linear, small_encoder_linear = (my_encoder, small_model_encoder) \\\n",
    "                if isinstance(my_encoder, nn.Linear) else (my_encoder[-1], small_model_encoder[-1])\n",
    "            small_in_dim = small_encoder_linear.out_features\n",
    "            my_encoder_linear.weight.zero_()\n",
    "            my_encoder_linear.bias.zero_()\n",
    "            my_encoder_linear.weight[:small_in_dim] = small_encoder_linear.weight\n",
    "            my_encoder_linear.bias[:small_in_dim] = small_encoder_linear.bias\n",
    "\n",
    "        set_encoder_weights(self.encoder, small_model.encoder)\n",
    "        set_encoder_weights(self.y_encoder, small_model.y_encoder)\n",
    "\n",
    "        small_in_dim = small_model.decoder.in_features\n",
    "\n",
    "        self.decoder.weight[:, :small_in_dim] = small_model.decoder.weight\n",
    "        self.decoder.bias = small_model.decoder.bias\n",
    "\n",
    "        for my_layer, small_layer in zip(self.transformer_encoder.layers, small_model.transformer_encoder.layers):\n",
    "            small_hid_dim = small_layer.linear1.out_features\n",
    "            my_in_dim = my_layer.linear1.in_features\n",
    "\n",
    "            # packed along q,k,v order in first dim\n",
    "            my_in_proj_w = my_layer.self_attn.in_proj_weight\n",
    "            small_in_proj_w = small_layer.self_attn.in_proj_weight\n",
    "\n",
    "            my_in_proj_w.view(3, my_in_dim, my_in_dim)[:, :small_in_dim, :small_in_dim] = small_in_proj_w.view(3,\n",
    "                                                                                                               small_in_dim,\n",
    "                                                                                                               small_in_dim)\n",
    "            my_layer.self_attn.in_proj_bias.view(3, my_in_dim)[:,\n",
    "            :small_in_dim] = small_layer.self_attn.in_proj_bias.view(3, small_in_dim)\n",
    "\n",
    "            my_layer.self_attn.out_proj.weight[:small_in_dim, :small_in_dim] = small_layer.self_attn.out_proj.weight\n",
    "            my_layer.self_attn.out_proj.bias[:small_in_dim] = small_layer.self_attn.out_proj.bias\n",
    "\n",
    "            my_layer.linear1.weight[:small_hid_dim, :small_in_dim] = small_layer.linear1.weight\n",
    "            my_layer.linear1.bias[:small_hid_dim] = small_layer.linear1.bias\n",
    "\n",
    "            my_layer.linear2.weight[:small_in_dim, :small_hid_dim] = small_layer.linear2.weight\n",
    "            my_layer.linear2.bias[:small_in_dim] = small_layer.linear2.bias\n",
    "\n",
    "            my_layer.norm1.weight[:small_in_dim] = math.sqrt(small_in_dim / my_in_dim) * small_layer.norm1.weight\n",
    "            my_layer.norm2.weight[:small_in_dim] = math.sqrt(small_in_dim / my_in_dim) * small_layer.norm2.weight\n",
    "\n",
    "            my_layer.norm1.bias[:small_in_dim] = small_layer.norm1.bias\n",
    "            my_layer.norm2.bias[:small_in_dim] = small_layer.norm2.bias\n",
    "\n",
    "\n",
    "class TransformerEncoderDiffInit(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer_creator: a function generating objects of TransformerEncoderLayer class without args (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer_creator, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer_creator() for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_encoder_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8aa0f2395600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = TransformerModel(encoder, n_out, emsize, nhead, nhid, nlayers, dropout, style_encoder=style_encoder,\n\u001b[1;32m      2\u001b[0m                              \u001b[0my_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_encoder_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_normalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                              \u001b[0mpos_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_encoder_generator\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpositional_encodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoPositionalEncoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                              \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mefficient_eval_masking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mefficient_eval_masking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_extra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_encoder_generator' is not defined"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(encoder, n_out, emsize, nhead, nhid, nlayers, dropout, style_encoder=style_encoder,\n",
    "                             y_encoder=y_encoder_generator(1, emsize), input_normalization=input_normalization,\n",
    "                             pos_encoder=(pos_encoder_generator or positional_encodings.NoPositionalEncoding)(emsize, bptt*2),\n",
    "                             decoder=decoder, init_method=initializer, efficient_eval_masking=efficient_eval_masking, **model_extra_args\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.criterion = criterion\n",
    "if load_weights_from_this_state_dict is not None:\n",
    "    model.load_state_dict(load_weights_from_this_state_dict)\n",
    "if initialize_with_model is not None:\n",
    "    model.init_from_small_model(initialize_with_model)\n",
    "\n",
    "print(f\"Using a Transformer with {sum(p.numel() for p in model.parameters())/1000/1000:.{2}f} M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1917\n"
     ]
    }
   ],
   "source": [
    "print(full_len)\n",
    "print(len(x_src) - single_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_mask_to_att_mask(mask):\n",
    "    return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask1 = bool_mask_to_att_mask(torch.ones((full_len, full_len), dtype=torch.bool)).to(x_src.device)\n",
    "src_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask2 = single_eval_pos + len(style_src)\n",
    "src_mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, 0., -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, 0.]])"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't get it: why we do len(x_src) - single_eval_pos?\n",
    "src_mask3 = generate_D_q_matrix(full_len, len(x_src) - single_eval_pos).to(x_src.device)\n",
    "src_mask3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
